

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Latent Reinforcement Learning &mdash; DI-drive 0.3.4 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="MetaDrive Macro Environment" href="md_macro.html" />
    <link rel="prev" title="End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances" href="implicit.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-drive
          

          
          </a>

          
            
            
              <div class="version">
                0.3.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorial/index.html">Tutorial</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../features/index.html">Features</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Model Zoo</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#imitation-learning-model-zoo">Imitation Learning Model Zoo</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#reinforcement-learning-model-zoo">Reinforcement Learning Model Zoo</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="simple_rl.html">BeV Speed End-to-end Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="implicit.html">End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Latent Reinforcement Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#training-auto-encoder">Training auto-encoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="#trainging-rl-agent">Trainging RL agent</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="md_macro.html">MetaDrive Macro Environment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#other-method">Other Method</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-drive</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Model Zoo</a> &raquo;</li>
        
      <li>Latent Reinforcement Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/model_zoo/latent_rl.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="latent-reinforcement-learning">
<h1>Latent Reinforcement Learning<a class="headerlink" href="#latent-reinforcement-learning" title="Permalink to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<p>Latent Reinforcement Learning is a category of policy settings which generate
latent representations that can completely characterize the features related to
the driving task in the input observation. Technically, it uses a Variational
Auto-encoder (VAE) to get latent embeddings, and takes it as observation to
train RL policies.</p>
<p>Our implementation refers to <a class="reference external" href="https://arxiv.org/abs/1904.09503">Model-free RL for AD</a>
with <strong>DI-drive</strong> and <strong>DI-engine</strong> and transfers it to more general cases. All
entries can be found in <code class="docutils literal notranslate"><span class="pre">demo/latent_rl</span></code>. In the
following documents, it will be explained in detail.</p>
<section id="training-auto-encoder">
<h2>Training auto-encoder<a class="headerlink" href="#training-auto-encoder" title="Permalink to this heading">¶</a></h2>
<p>The first step is to train a VAE to encode input image
into a imbedding feature. We deploy the model and training following
open source VAE <a class="reference external" href="https://github.com/AntixK/PyTorch-VAE">vae</a>.
..You can also use pretrained model vae.ckpt.
We follow the standard setting. The input of the auto-encoder module
is birdview image, whose shape is 192 * 192 * 7. We use 5
convolutional layers activated with leakyReLU as the encoder, which
encode the input birdview image to a embedding with 128 channels.
The decoder shares the same convolution kernel size and activation
layer, but uses transposed convolutional layer. Using the encoder,
we can convert the input birdview image to 128-channel embedding,
which helps the training of RL agent.</p>
<p>The training procedure includes collecting birdview data and training the
auto-encoder.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>demo/latent_rl
<span class="c1"># Collect data</span>
python<span class="w"> </span>collect_data.py
<span class="c1"># Train VAE</span>
python<span class="w"> </span>train_vae.py
</pre></div>
</div>
<p>You can custom the configurations in the files above.</p>
</section>
<section id="trainging-rl-agent">
<h2>Trainging RL agent<a class="headerlink" href="#trainging-rl-agent" title="Permalink to this heading">¶</a></h2>
<p>Following Model-free RL for AD , we use DDQN as the
discrete RL algorithm and TD3 as the continuous RL algorithm.
For DDQN, we discrete the steering and throttle to 10 class, which
means the whole action space is 100. The steering and throttle is
mapping to [-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8] and
[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] respectively.
although the action space is discrete, the vehicle can run smoothly when
turning.
For TD3, we use continuous action space. The policy network’s output is
the mean and log scale standard deviation of a Gaussian distribution.
At each state of collecting data, we sample an action following the
generated Gaussian distribution. For TD3, we found that the steering
changes drastically. The steering is -1 or 0.8 and the throttle is
always 0.9. However, the car can drive well. This is a point to be
improved.</p>
<p>The training, evaluation and testing of the method mentioned above is
provided. We use the standard policy settings of <strong>DI-engine</strong>. You can
see their doc to get how to modify the training settings. You may need
to change the Carla server setting in the entry files.</p>
<p>DDQN:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train DDQN agent</span>
python<span class="w"> </span>latent_dqn_train.py
<span class="c1"># Benchmark evaluation</span>
python<span class="w"> </span>latent_dqn_eval.py
<span class="c1"># Test and visualize</span>
python<span class="w"> </span>latent_dqn_test.py
</pre></div>
</div>
<p><a class="reference external" href="https://huggingface.co/OpenDILabCommunity/DI-drive/resolve/main/latent_rl.ckpt?download=true">latent_rl.ckpt</a></p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="md_macro.html" class="btn btn-neutral float-right" title="MetaDrive Macro Environment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="implicit.html" class="btn btn-neutral float-left" title="End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>