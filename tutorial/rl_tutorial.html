

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Simple Reinforcement Learning &mdash; DI-drive 0.3.4 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Features" href="../features/index.html" />
    <link rel="prev" title="Simple Imitation Learning" href="il_tutorial.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-drive
          

          
          </a>

          
            
            
              <div class="version">
                0.3.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorial</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="core_concepts.html">Core Concepts and Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="carla_tutorial.html">Carla tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_run.html">Auto policy running and visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="il_tutorial.html">Simple Imitation Learning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Simple Reinforcement Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#carla-rl-tutorial">Carla RL Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#rl-training-using-di-engine">RL training using DI-engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="#evaluate-and-test-the-trained-model">Evaluate and test the trained model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#metadrive-rl-tutorial">MetaDrive RL Tutorial</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../features/index.html">Features</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-drive</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Tutorial</a> &raquo;</li>
        
      <li>Simple Reinforcement Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial/rl_tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="simple-reinforcement-learning">
<h1>Simple Reinforcement Learning<a class="headerlink" href="#simple-reinforcement-learning" title="Permalink to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<p><strong>DI-drive</strong> + <strong>DI-engine</strong> make RL for Autonomous Driving very easy. Here we show how to
use <strong>DI-drive</strong> to run a simple Reinforcement Learning driving policy with Carla and MetaDrive
separately.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h2>
<p>Ubuntu 16.04 system + Intel(R) Core(TM) i7-8700 CPU &#64; 3.20GHz + 32G
memory + GPU1060</p>
</section>
<section id="carla-rl-tutorial">
<h2>Carla RL Tutorial<a class="headerlink" href="#carla-rl-tutorial" title="Permalink to this heading">¶</a></h2>
<p>DI-drive support several RL policies and provide a simple RL env running with Carla server.
The policy takes a small Bird-eye View image together with
current speed as input, and directly outputs control signals.</p>
<section id="rl-training-using-di-engine">
<h3>RL training using DI-engine<a class="headerlink" href="#rl-training-using-di-engine" title="Permalink to this heading">¶</a></h3>
<p>We build simple RL demos that can run varies RL algorithm with the aforementioned simple environment setting.
All the code can be found in <code class="docutils literal notranslate"><span class="pre">demo/simple_rl</span></code>, including training, evaluating and testing.</p>
<p>Here we show how to run the DQN demo. It follows the standard deployment of a DI-engine RL entry.
Other RL policies run in same way.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>demo/simple_rl
python<span class="w"> </span>simple_rl_train.py<span class="w"> </span>-p<span class="w"> </span>dqn
</pre></div>
</div>
<p>The config part defines the env and policy settings. Notes that you need to change the Carla server
host and port, and modify the environment nums according to yours. By default it uses 8 Carla servers on
<cite>localhost</cite> with port from 9000 to 9016.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">exp_name</span><span class="o">=...</span><span class="p">,</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="o">...</span>
    <span class="p">),</span>
    <span class="n">server</span><span class="o">=</span><span class="p">[</span>
        <span class="nb">dict</span><span class="p">(</span><span class="n">carla_host</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span> <span class="n">carla_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">9000</span><span class="p">,</span> <span class="mi">9016</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
    <span class="p">],</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="o">...</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For more details about how to tune parameters in DQN, you can see <strong>DI-engine</strong>’s doc. Usually
you may concern about the replay buffer size and sample num per collection.</p>
<p>When you see the information in terminal that contains the content in
the following picture, it means that you are beginning to train the
model.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/rl_tutorial_log.png"><img alt="rl_tutorial_log" src="../_images/rl_tutorial_log.png" style="width: 1000px;" /></a>
</figure>
<p>In the process of training, you can use the tensorboard as a monitor,
the default log path is in your working directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tensorboard<span class="w"> </span>--logdir<span class="o">=</span><span class="s1">&#39;./log&#39;</span>
</pre></div>
</div>
<p>After running for about 24 hours, you will get:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/rl_tutorial_tb.png"><img alt="rl_tutorial_tb" src="../_images/rl_tutorial_tb.png" style="width: 800px;" /></a>
</figure>
</section>
<section id="evaluate-and-test-the-trained-model">
<h3>Evaluate and test the trained model<a class="headerlink" href="#evaluate-and-test-the-trained-model" title="Permalink to this heading">¶</a></h3>
<p>After training, you can evaluate the trained model on a benchmark suite. Simply run the following code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>simple_rl_eval.py<span class="w"> </span>-p<span class="w"> </span>dqn<span class="w"> </span>-c<span class="w"> </span>PATH_TO_YOUR_CKPT
</pre></div>
</div>
<p>You may need to change Carla server numbers and settints, change the suite you want to evaluate, and add
your pre-trained weights in policy’s config.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">eval_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">env_num</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="o">...</span>
    <span class="p">),</span>
    <span class="n">server</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">carla_host</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span>
        <span class="n">carla_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">9000</span><span class="p">,</span> <span class="mi">9010</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="p">)],</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">cuda</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">ckpt_path</span><span class="o">=</span><span class="s1">&#39;path/to/your/model&#39;</span><span class="p">,</span>
        <span class="nb">eval</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">evaluator</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">suite</span><span class="o">=</span><span class="s1">&#39;FullTown02-v1&#39;</span><span class="p">,</span>
                <span class="o">...</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">...</span>
    <span class="p">),</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The default DQN policy can have nice probability to complete navigation in <cite>FullTown02-v2</cite>, with traffic lights
ignored.</p>
<p>Also, you can test the policy in a town route with a visualized screen. Simply run the following code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>simple_rl_test.py<span class="w"> </span>-p<span class="w"> </span>dqn<span class="w"> </span>-c<span class="w"> </span>PATH_TO_YOUR_CKPT
</pre></div>
</div>
<p>You may need to change Carla server settints, switch on/off visualization or save a replay gif/video
and add your pre-trained weights in policy’s config.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="o">...</span>
        <span class="n">visualize</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;birdview&#39;</span><span class="p">,</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;show&#39;</span><span class="p">],</span> <span class="c1"># or &#39;gif&#39;, &#39;video&#39;</span>
            <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
            <span class="n">frame_skip</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># avoid to be too large</span>
        <span class="p">),</span>
    <span class="p">),</span>
    <span class="n">server</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">carla_host</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span>
        <span class="n">carla_ports</span><span class="o">=</span><span class="p">[</span><span class="mi">9000</span><span class="p">,</span> <span class="mi">9002</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="p">)],</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">cuda</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">ckpt_path</span><span class="o">=</span><span class="s1">&#39;path/to/your/model&#39;</span><span class="p">,</span>
        <span class="nb">eval</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">evaluator</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                <span class="n">render</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="o">...</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="o">...</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="metadrive-rl-tutorial">
<h2>MetaDrive RL Tutorial<a class="headerlink" href="#metadrive-rl-tutorial" title="Permalink to this heading">¶</a></h2>
<p>DI-drive provide a simple entry that can run MetaDrive default environments
with DI-engine policies. The training entry can be found in <cite>demo/metadrive</cite></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>demo/metadrive
python<span class="w"> </span>basic_env_train.py
</pre></div>
</div>
<p>MetaDrive has standard <cite>gym</cite> environments. Adding an <code class="docutils literal notranslate"><span class="pre">EnvWrapper</span></code> together with
other components and pipeline in DI-engine, RL experiments is able to run.</p>
<p>Here, we use “MetaDrive-1000envs-v0” to train and “MetaDrive-validation-v0” to evaluate
an on-policy PPO policy. You can modify the env num in config to suit your device.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metadrive_basic_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="o">...</span>
        <span class="n">collector_env_num</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">evaluator_env_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../features/index.html" class="btn btn-neutral float-right" title="Features" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="il_tutorial.html" class="btn btn-neutral float-left" title="Simple Imitation Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>